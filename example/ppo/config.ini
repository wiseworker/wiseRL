[ppo]
# network setting

net_dims = (256, 128) 
# training setting
max_train_steps = 200000
evaluate_freq = 5000
save_freq = 20
batch_size = 2048
mini_batch_size = 64 
hidden_width = 128
optimizer = Adam
lr_a = 0.005
lr_c = 0.005
gamma = 0.99
lamda  = 0.95
epsilon = 0.2
K_epochs = 10
use_adv_norm=True  
use_state_norm = True 
use_reward_norm=False 
use_reward_scaling = True 
entropy_coef =0.01 
use_lr_decay=True
use_grad_clip=True 
use_orthogonal_init=True 
set_adam_eps=True
use_tanh = True 